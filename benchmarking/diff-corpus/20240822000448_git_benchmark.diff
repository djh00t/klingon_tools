diff --git a/benchmarking/diff-corpus/20240822000233_git_benchmark.diff b/benchmarking/diff-corpus/20240822000233_git_benchmark.diff
new file mode 100644
index 0000000..f066511
--- /dev/null
+++ b/benchmarking/diff-corpus/20240822000233_git_benchmark.diff
@@ -0,0 +1,480 @@
+diff --git a/benchmarking/diff-corpus/20240822000111_git_benchmark.diff b/benchmarking/diff-corpus/20240822000111_git_benchmark.diff
+new file mode 100644
+index 0000000..64c4f12
+--- /dev/null
++++ b/benchmarking/diff-corpus/20240822000111_git_benchmark.diff
+@@ -0,0 +1,474 @@
++diff --git a/benchmarking/diff-corpus/20240821235818_git_benchmark.diff b/benchmarking/diff-corpus/20240821235818_git_benchmark.diff
++new file mode 100644
++index 0000000..b319f98
++--- /dev/null
+++++ b/benchmarking/diff-corpus/20240821235818_git_benchmark.diff
++@@ -0,0 +1,468 @@
+++diff --git a/benchmarking/diff-corpus/20240821225825_git_benchmark.diff b/benchmarking/diff-corpus/20240821225825_git_benchmark.diff
+++new file mode 100644
+++index 0000000..1b3348c
+++--- /dev/null
++++++ b/benchmarking/diff-corpus/20240821225825_git_benchmark.diff
+++@@ -0,0 +1,462 @@
++++diff --git a/benchmarking/archive/conventional_commit_benchmark.py b/benchmarking/archive/conventional_commit_benchmark.py
++++new file mode 100644
++++index 0000000..ffb2352
++++--- /dev/null
+++++++ b/benchmarking/archive/conventional_commit_benchmark.py
++++@@ -0,0 +1,456 @@
+++++# benchmarking/conventional_commit_benchmark.py
+++++# flake8: noqa: E501
+++++import re
+++++import sys
+++++import time
+++++import logging
+++++from typing import List
+++++import json
+++++from tabulate import tabulate
+++++import litellm
+++++
+++++# Set the logging level for LiteLLM, requests, urllib3 and httpx to
+++++# WARNING to prevent excessive stdout logging
+++++logging.getLogger("LiteLLM").setLevel(logging.WARNING)
+++++logging.getLogger("requests").setLevel(logging.WARNING)
+++++logging.getLogger("urllib3").setLevel(logging.WARNING)
+++++logging.getLogger("httpx").setLevel(logging.WARNING)
+++++
+++++# List of models to benchmark
+++++models_to_test = [
+++++    #    "gpt-4o",
+++++    #    "gpt-4o-mini",
+++++    #    "chatgpt-4o-latest",
+++++    #    "claude-3-5-sonnet-20240620",
+++++    #    "claude-3-haiku-20240307",
+++++    #    "mistral/codestral-latest",
+++++    #    "mistral/mistral-small-latest",
+++++    # "groq/llama2-70b-4096",
+++++    #    "groq/llama3-8b-8192",
+++++    #    "groq/llama3-70b-8192",
+++++    #    "groq/llama-3.1-8b-instant",
+++++    #    "groq/llama-3.1-70b-versatile",
+++++    # "groq/llama-3.1-405b-reasoning",
+++++    #    "groq/mixtral-8x7b-32768",
+++++    "groq/gemma-7b-it",
+++++    #    "groq/llama3-groq-70b-8192-tool-use-preview",
+++++    #    "groq/llama3-groq-8b-8192-tool-use-preview",
+++++    #    "ollama_chat/codegemma:2b",
+++++    #    "ollama_chat/deepseek-coder-v2",
+++++    #    "ollama_chat/phi3:mini",
+++++    #    "ollama_chat/mistral-nemo",
+++++    #    "ollama_chat/llama2:latest",
+++++    #    "ollama_chat/llama3.1:latest",
+++++    #    "ollama_chat/codeqwen:latest",
+++++    #    "ollama_chat/codellama:latest",
+++++]
+++++
+++++# Initialize logging
+++++logging.basicConfig(level=logging.INFO)
+++++logger = logging.getLogger(__name__)
+++++
+++++
+++++def read_static_diff(file_path: str) -> str:
+++++    """Read the diff file to be used for benchmarking."""
+++++    try:
+++++        with open(file_path, "r") as f:
+++++            return f.read()
+++++    except FileNotFoundError:
+++++        logger.error(f"Diff file not found: {file_path}")
+++++        sys.exit(1)
+++++
+++++
+++++def extract_content_within_backticks(text: str) -> str:
+++++    """
+++++    Extracts content enclosed in single/triple backticks, double asterisks (**), or double quotes (").
+++++    If any of these delimiters contain content, it is returned.
+++++    Prioritizes triple backticks > single backticks > double asterisks > double quotes.
+++++    """
+++++    # Match content within triple backticks
+++++    triple_backtick_match = re.search(
+++++        r"```(?:[^\n]*\n)?(.*?)\n?```", text, re.DOTALL
+++++    )
+++++    if triple_backtick_match:
+++++        return triple_backtick_match.group(1).strip()
+++++
+++++    # Match content within single backticks
+++++    single_backtick_match = re.search(r"`(.*?)`", text, re.DOTALL)
+++++    if single_backtick_match:
+++++        return single_backtick_match.group(1).strip()
+++++
+++++    # Match content within double asterisks
+++++    asterisk_match = re.search(r"\*\*(.*?)\*\*", text, re.DOTALL)
+++++    if asterisk_match:
+++++        return asterisk_match.group(1).strip()
+++++
+++++    # Match content within double quotes
+++++    double_quote_match = re.search(r'"(.*?)"', text, re.DOTALL)
+++++    if double_quote_match:
+++++        return double_quote_match.group(1).strip()
+++++
+++++    # Return the original if no matches are found
+++++    return text.strip()
+++++
+++++
+++++def generate_commit_message_raw(model: str, diff: str) -> str:
+++++    """Generate a raw commit message using the model based on the provided diff."""
+++++    try:
+++++        response = litellm.completion(
+++++            model=model,
+++++            messages=[
+++++                {
+++++                    "role": "system",
+++++                    "content": """
+++++                    Generate a commit message based solely on the git diff
+++++                    provided, ensuring accuracy and relevance to the actual
+++++                    changes. Avoid speculative or unnecessary footers, such as
+++++                    references to non-existent issues.
+++++
+++++                    Follow the Conventional Commits standard using the following
+++++                    format: `<type>(scope): <description>`
+++++
+++++                    Consider the following options when selecting commit types:
+++++                    • build: updates to build system & external dependencies
+++++                    • chore: changes that don't modify src or test files
+++++                    • ci: changes to CI configuration files and scripts
+++++                    • docs: updates to documentation & comments
+++++                    • feat: add new feature or function to the codebase
+++++                    • fix: correct bugs and other errors in code
+++++                    • perf: improve performance without changing existing functionality
+++++                    • refactor: code changes that neither fix bugs nor add features
+++++                    • revert: Reverts a previous commit
+++++                    • style: changes that do not affect the meaning of the code
+++++                    (white-space, formatting, missing semi-colons, etc) but improve
+++++                    readability, consistency, or maintainability
+++++                    • test: add, update, correct unit tests
+++++                    • other:  Changes that don't fit into the above categories
+++++
+++++                    Scope: Select the most specific of application name, file name,
+++++                    class name, method/function name, or feature name for the commit
+++++                    scope. If in doubt, use the name of the file being modified.
+++++
+++++                    Breaking Changes: Include a `BREAKING CHANGE:` footer or append !
+++++                    after type/scope for commits that introduce breaking changes.
+++++
+++++                    Footers: Breaking change is the only footer permitted. Do not add
+++++                    "Co-authored-by" or other footers unless explicitly
+++++                    requested.
+++++
+++++                    If you must respond with more than just the commit message,
+++++                    ensure that you enclose it in triple backticks to avoid any issues.
+++++
+++++                    Remember it is critical that you follow the Conventional
+++++                    Commits standard and do not return any other content before
+++++                    or after the commit message:
+++++                    `<type>(scope): <description>`
+++++                    """,
+++++                },
+++++                {
+++++                    "role": "user",
+++++                    "content": f'Generate a git conventional commit message based on this diff: "{diff}"',
+++++                },
+++++            ],
+++++        )
+++++        return response.choices[0].message.content.strip()
+++++    except Exception as e:
+++++        logger.error(
+++++            f"Failed to generate commit message with model {model}: {e}"
+++++        )
+++++        return ""
+++++
+++++
+++++def evaluate_compliance_and_quality(
+++++    model: str, raw_commit_message: str, cleaned_commit_message: str, diff: str
+++++):
+++++    """Evaluate the compliance and quality of the commit message using GPT-4o."""
+++++    try:
+++++        compliance_response = litellm.completion(
+++++            model=model,
+++++            messages=[
+++++                {
+++++                    "role": "system",
+++++                    "content": """
+++++                Evaluate the following raw and cleaned conventional commit messages
+++++                based on the following criteria. Provide a score out of 10 for each
+++++                message (commit_score_raw and commit_score_clean) using the scoring system
+++++                below:
+++++
+++++                1. The commit message must fit the following format: `<type>(scope): <description>` (2 points)
+++++                2. The <type> value must be one of the commit types: build, chore, ci, docs, feat, fix, perf, refactor, revert, style, test, other (1 point)
+++++                3. The (scope) value must be one of application name, file name, class name, method/function name, or feature name. If in doubt, use the name of the file being modified. (1 point)
+++++                4. The first line of the description must explain the change at a high level. (1 point)
+++++                5. Any lines following the first line of the commit must explain any changes not covered by the first line or explain the change in more detail. (1 point)
+++++                6. The commit message must not contain any details that are inaccurate. (2 points)
+++++                7. The commit message must be the only content returned by the model or surrounded by triple backticks. (2 points)
+++++
+++++                Score both the raw and cleaned commit messages using these
+++++                criteria. Return the scores in the following schema:
+++++                ```json
+++++                {
+++++                    "$schema": "http://json-schema.org/draft-07/schema#",
+++++                    "title": "Commit Message Scoring Schema",
+++++                    "type": "object",
+++++                    "properties": {
+++++                        "format_compliance": {
+++++                        "type": "integer",
+++++                        "description": "The commit message must fit the format: <type>(scope): <description>",
+++++                        "minimum": 0,
+++++                        "maximum": 2
+++++                        },
+++++                        "type_compliance": {
+++++                        "type": "integer",
+++++                        "description": "The <type> value must be one of the allowed commit types.",
+++++                        "minimum": 0,
+++++                        "maximum": 1
+++++                        },
+++++                        "scope_compliance": {
+++++                        "type": "integer",
+++++                        "description": "The (scope) value must be one of the allowed scope types.",
+++++                        "minimum": 0,
+++++                        "maximum": 1
+++++                        },
+++++                        "high_level_description": {
+++++                        "type": "integer",
+++++                        "description": "The first line of the description must explain the change at a high level.",
+++++                        "minimum": 0,
+++++                        "maximum": 1
+++++                        },
+++++                        "detailed_description": {
+++++                        "type": "integer",
+++++                        "description": "Any lines following the first must explain changes not covered by the first or elaborate on the first line.",
+++++                        "minimum": 0,
+++++                        "maximum": 1
+++++                        },
+++++                        "accuracy_compliance": {
+++++                        "type": "integer",
+++++                        "description": "The commit message must not contain any inaccurate details.",
+++++                        "minimum": 0,
+++++                        "maximum": 2
+++++                        },
+++++                        "content_compliance": {
+++++                        "type": "integer",
+++++                        "description": "The commit message must be the only content returned or enclosed in triple backticks.",
+++++                        "minimum": 0,
+++++                        "maximum": 2
+++++                        },
+++++                        "total_score": {
+++++                        "type": "integer",
+++++                        "description": "The total score for the commit message, which should be the sum of the individual scores.",
+++++                        "minimum": 0,
+++++                        "maximum": 10
+++++                        }
+++++                    },
+++++                    "required": [
+++++                        "format_compliance",
+++++                        "type_compliance",
+++++                        "scope_compliance",
+++++                        "high_level_description",
+++++                        "detailed_description",
+++++                        "accuracy_compliance",
+++++                        "content_compliance",
+++++                        "total_score"
+++++                    ]
+++++                }
+++++                ```
+++++                """,
+++++                },
+++++                {
+++++                    "role": "user",
+++++                    "content": f"Raw commit message: {raw_commit_message}\nCleaned commit message: {cleaned_commit_message}\nDiff: {diff}",
+++++                },
+++++            ],
+++++        )
+++++        print()
+++++        print(compliance_response)
+++++        print()
+++++        compliance_result = compliance_response.choices[
+++++            0
+++++        ].message.content.strip()
+++++        print()
+++++        print(compliance_result)
+++++        print()
+++++        # Parse out compliance results
+++++        raw_compliance = 1 if "Compliance: Pass" in compliance_result else 0
+++++        cleaned_compliance = (
+++++            1 if "Compliance: Pass" in compliance_result else 0
+++++        )
+++++
+++++        # Extract quality score
+++++        quality_match = re.search(r"Quality: (\d+)/10", compliance_result)
+++++        response_quality = (
+++++            float(quality_match.group(1)) if quality_match else 7.0
+++++        )  # Default to 7 if not found
+++++
+++++        return raw_compliance, cleaned_compliance, response_quality
+++++    except Exception as e:
+++++        logger.error(
+++++            f"Failed to evaluate compliance and quality with model {model}: {e}"
+++++        )
+++++        return 0, 0, 7.0  # Default to fail and a score of 7
+++++
+++++
+++++def calculate_time_score(time_taken, max_time):
+++++    return 1 - (time_taken / max_time)
+++++
+++++
+++++def calculate_compliance_score(raw_compliance, cleaned_compliance):
+++++    return raw_compliance + cleaned_compliance
+++++
+++++
+++++def normalize_rank(rank, model_count):
+++++    return 1 - (rank / model_count)
+++++
+++++
+++++def calculate_overall_score(
+++++    time_taken,
+++++    max_time,
+++++    raw_compliance,
+++++    cleaned_compliance,
+++++    response_quality,
+++++    speed_rank,
+++++    quality_rank,
+++++    model_count,
+++++    weights=None,
+++++):
+++++    if weights is None:
+++++        weights = {
+++++            "time_weight": 0.20,
+++++            "raw_compliance_weight": 0.15,
+++++            "cleaned_compliance_weight": 0.15,
+++++            "response_quality_weight": 0.30,
+++++            "speed_rank_weight": 0.10,
+++++            "quality_rank_weight": 0.10,
+++++        }
+++++
+++++    time_score = calculate_time_score(time_taken, max_time)
+++++
+++++    normalized_speed_rank = normalize_rank(speed_rank, model_count)
+++++    normalized_quality_rank = normalize_rank(quality_rank, model_count)
+++++
+++++    overall_score = (
+++++        weights["time_weight"] * time_score
+++++        + weights["raw_compliance_weight"] * raw_compliance
+++++        + weights["cleaned_compliance_weight"] * cleaned_compliance
+++++        + weights["response_quality_weight"]
+++++        * (response_quality / 10)  # Normalize quality to [0,1]
+++++        + weights["speed_rank_weight"] * normalized_speed_rank
+++++        + weights["quality_rank_weight"] * normalized_quality_rank
+++++    )
+++++
+++++    return overall_score
+++++
+++++
+++++def benchmark_models(diff: str, models: List[str]):
+++++    results = []
+++++    max_time_taken = 0  # To store max time for normalization later
+++++
+++++    for model in models:
+++++        start_time = time.time()
+++++        raw_commit_message = generate_commit_message_raw(model, diff)
+++++        cleaned_commit_message = extract_content_within_backticks(
+++++            raw_commit_message
+++++        )
+++++        end_time = time.time()
+++++        elapsed_time = end_time - start_time
+++++
+++++        # Store the max time for normalization purposes
+++++        if elapsed_time > max_time_taken:
+++++            max_time_taken = elapsed_time
+++++
+++++        # Log details for each model as the benchmarking progresses
+++++        logger.info(f"Model: {model}")
+++++        logger.info(f"Time taken: {elapsed_time:.2f} seconds")
+++++        logger.info("-" * 80)
+++++        logger.info("Raw commit message: ")
+++++        logger.info(f"{raw_commit_message}")
+++++        logger.info("-" * 80)
+++++        logger.info("Cleaned commit message: ")
+++++        logger.info(f"{cleaned_commit_message}")
+++++        logger.info("=" * 80)
+++++
+++++        # Evaluate compliance and quality dynamically
+++++        raw_compliance, cleaned_compliance, response_quality = (
+++++            evaluate_compliance_and_quality(
+++++                model, raw_commit_message, cleaned_commit_message, diff
+++++            )
+++++        )
+++++
+++++        results.append(
+++++            {
+++++                "model": model,
+++++                "time_taken": elapsed_time,
+++++                "raw_compliance": raw_compliance,
+++++                "cleaned_compliance": cleaned_compliance,
+++++                "response_quality": response_quality,
+++++                "speed_rank": 1,  # Placeholder rank, to be calculated later
+++++                "quality_rank": 1,  # Placeholder rank, to be calculated later
+++++            }
+++++        )
+++++
+++++    # Assign speed and quality ranks
+++++    results.sort(key=lambda x: x["time_taken"])
+++++    for i, result in enumerate(results):
+++++        result["speed_rank"] = i + 1
+++++
+++++    results.sort(key=lambda x: x["response_quality"], reverse=True)
+++++    for i, result in enumerate(results):
+++++        result["quality_rank"] = i + 1
+++++
+++++    # Calculate overall score
+++++    for result in results:
+++++        result["overall_score"] = calculate_overall_score(
+++++            result["time_taken"],
+++++            max_time_taken,
+++++            result["raw_compliance"],
+++++            result["cleaned_compliance"],
+++++            result["response_quality"],
+++++            result["speed_rank"],
+++++            result["quality_rank"],
+++++            len(models),
+++++        )
+++++
+++++    return results
+++++
+++++
+++++if __name__ == "__main__":
+++++    # Use a static diff file for benchmarking
+++++    diff_file_path = "20240821_git_benchmark.diff"
+++++    diff_content = read_static_diff(diff_file_path)
+++++
+++++    benchmark_results = benchmark_models(diff_content, models_to_test)
+++++
+++++    # Write results to a JSON file
+++++    with open("benchmark_results.json", "w") as f:
+++++        json.dump(benchmark_results, f, indent=4)
+++++
+++++    # Prepare data for table display
+++++    table_data = [
+++++        [
+++++            result["model"],
+++++            f'{result["time_taken"]:.2f} seconds',
+++++            result["raw_compliance"],
+++++            result["cleaned_compliance"],
+++++            result["response_quality"],
+++++            result["speed_rank"],
+++++            result["quality_rank"],
+++++            f'{result["overall_score"]:.2f}',
+++++        ]
+++++        for result in benchmark_results
+++++    ]
+++++    headers = [
+++++        "Model Name",
+++++        "Time Taken",
+++++        "Raw Format Compliance",
+++++        "Clean Format Compliance",
+++++        "Response Quality",
+++++        "Speed Rank",
+++++        "Quality Rank",
+++++        "Overall Score",
+++++    ]
+++++
+++++    # Display the results in table format
+++++    print(tabulate(table_data, headers, tablefmt="grid"))
+++++
+++++    # Write Markdown table to file
+++++    with open("benchmark_results.md", "w") as f:
+++++        f.write(tabulate(table_data, headers, tablefmt="pipe"))
